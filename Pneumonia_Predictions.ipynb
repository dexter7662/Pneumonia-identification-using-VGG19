{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pneumonia Predictions",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMxbPflPS3VMAkwS3/gITIV",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dexter7662/Pneumonia-identification-using-VGG19/blob/main/Pneumonia_Predictions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dua4rW3LLY56"
      },
      "source": [
        "Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUy4i7WQWSjd"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import recall_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.initializers import RandomNormal\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.layers import Flatten"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNJ6Z_NzLglw"
      },
      "source": [
        "Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVldmQNYLVUA"
      },
      "source": [
        "os.environ['KAGGLE_USERNAME'] = \"lorddexter\"\n",
        "os.environ['KAGGLE_KEY'] = \"281706d18d96f899cdc0f5617516c67a\"\n",
        "!kaggle datasets download -d khoongweihao/covid19-xray-dataset-train-test-sets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mzxqd3L5WVDS"
      },
      "source": [
        "!unzip covid19-xray-dataset-train-test-sets.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vTVL18EYGUF"
      },
      "source": [
        "main = 'xray_dataset_covid19/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mStZV1ZLYo25"
      },
      "source": [
        "train_normal = os.listdir(main + 'train/NORMAL')\n",
        "train_pneumonia = os.listdir(main + 'train/PNEUMONIA')\n",
        "\n",
        "test_normal = os.listdir(main + 'test/NORMAL')\n",
        "test_pneumonia = os.listdir(main + 'test/PNEUMONIA')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHy_zfleLncr"
      },
      "source": [
        "Creating dataframes from directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWNZPWTCZjW5"
      },
      "source": [
        "def dataframe(filenames, classes):\n",
        "  return pd.DataFrame({'Filename': filenames, 'Label': [classes]*len(filenames)})\n",
        "\n",
        "train_normal_df = dataframe(train_normal, 'Normal')\n",
        "train_pneumonia_df = dataframe(train_pneumonia, 'Pneumonia')\n",
        "\n",
        "test_normal_df = dataframe(test_normal, 'Normal')\n",
        "test_pneumonia_df = dataframe(test_pneumonia, 'Pneumonia')\n",
        "\n",
        "train_df = pd.concat([train_normal_df, train_pneumonia_df], axis=0)\n",
        "test_df = pd.concat([test_normal_df, test_pneumonia_df], axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xj98wENnnO7l"
      },
      "source": [
        "def pathmaker(df_name, df, empty_list):\n",
        "    for i in df_name.values:\n",
        "        if i[1] == 'Normal':\n",
        "            empty_list.append(str(main + df + '/NORMAL/'+i[0]))\n",
        "        else:\n",
        "            empty_list.append(str(main + df + '/PNEUMONIA/'+i[0]))\n",
        "\n",
        "\n",
        "#Empty list to be passed in path maker\n",
        "train_path = []\n",
        "test_path = []\n",
        "\n",
        "#Assigning Path maker\n",
        "pathmaker(train_df, 'train', train_path)\n",
        "pathmaker(test_df, 'test', test_path)\n",
        "\n",
        "train_df['Path'] = train_path\n",
        "test_df['Path'] = test_path\n",
        "\n",
        "#Shuffling / Re-arranging rows\n",
        "train_df = train_df.sample(frac=1).reset_index(drop=True).iloc[:, 1:]\n",
        "test_df = test_df.sample(frac=1).reset_index(drop=True).iloc[:, 1:]\n",
        "\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJJcp-BiM0VF"
      },
      "source": [
        "Checking for any class inbalance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GT_ejq0UgPQ2"
      },
      "source": [
        "sns.set_theme(style=\"darkgrid\")\n",
        "sns.countplot(x='Label', data=train_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wvUBcYAb-K4"
      },
      "source": [
        "sns.set_theme(style=\"darkgrid\")\n",
        "sns.countplot(x='Label', data=test_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpLZfiuTLxw6"
      },
      "source": [
        "Creating the data input pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuhGAg4o6E-r"
      },
      "source": [
        "datagen = ImageDataGenerator(rescale=1./255., validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulfGe1xP6emb"
      },
      "source": [
        "train_generator = datagen.flow_from_dataframe(\n",
        "    dataframe=train_df,\n",
        "    directory=None,\n",
        "    x_col='Path',\n",
        "    y_col='Label',\n",
        "    subset='training',\n",
        "    batch_size=32,\n",
        "    seed=42,\n",
        "    shuffle=True,\n",
        "    class_mode='categorical',\n",
        "    target_size=(224, 224),\n",
        "    validate_filenames=False\n",
        ")\n",
        "\n",
        "validation_generator = datagen.flow_from_dataframe(\n",
        "    dataframe=train_df,\n",
        "    directory=None,\n",
        "    x_col='Path',\n",
        "    y_col='Label',\n",
        "    subset='validation',\n",
        "    batch_size=32,\n",
        "    seed=42,\n",
        "    shuffle=True,\n",
        "    class_mode='categorical',\n",
        "    target_size=(224, 224),\n",
        "    validate_filenames=False\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255.)\n",
        "test_generator = test_datagen.flow_from_dataframe(\n",
        "    dataframe=test_df,\n",
        "    directory=None,\n",
        "    x_col='Path',\n",
        "    y_col=None,\n",
        "    batch_size=32,\n",
        "    seed=42,\n",
        "    shuffle=False,\n",
        "    class_mode=None,\n",
        "    target_size=(224, 224),\n",
        "    validate_filenames=False\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvBKo1AXM-8G"
      },
      "source": [
        "Designing model architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9C6viwhHacE"
      },
      "source": [
        "input = Input(shape=(224,224,3))\n",
        "conv1 = Conv2D(filters=64,kernel_size=(3,3),strides=(1,1), padding='same', kernel_initializer= 'random_normal', bias_initializer= 'zeros')(input)\n",
        "conv2 = Conv2D(filters=64,kernel_size=(3,3),strides=(1,1), padding='same', kernel_initializer= 'random_normal', bias_initializer= 'zeros')(conv1)\n",
        "pool1 = MaxPooling2D(pool_size=(2,2), strides=2)(conv2)\n",
        "conv3 = Conv2D(filters=128, kernel_size=(3,3),strides=(1,1), padding='same', kernel_initializer= 'random_normal', bias_initializer= 'zeros')(pool1)\n",
        "conv4 = Conv2D(filters=128, kernel_size=(3,3),strides=(1,1), padding='same', kernel_initializer= 'random_normal', bias_initializer= 'zeros')(conv3)\n",
        "pool2 = MaxPooling2D(pool_size=(2,2), strides=2)(conv4)\n",
        "conv5 = Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='same', kernel_initializer= 'random_normal', bias_initializer= 'zeros')(pool2)\n",
        "conv6 = Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='same', kernel_initializer= 'random_normal', bias_initializer= 'zeros')(conv5)\n",
        "conv7 = Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='same', kernel_initializer= 'random_normal', bias_initializer= 'zeros')(conv6)\n",
        "pool3 = MaxPooling2D(pool_size=(2,2), strides=2)(conv7)\n",
        "conv8 = Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), padding='same', kernel_initializer= 'random_normal', bias_initializer= 'zeros')(pool3)\n",
        "conv9 = Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), padding='same', kernel_initializer= 'random_normal', bias_initializer= 'zeros')(conv8)\n",
        "conv10 = Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), padding='same', kernel_initializer= 'random_normal', bias_initializer= 'zeros')(conv9)\n",
        "pool4 = MaxPooling2D(pool_size= (2,2), strides= 2)(conv10)\n",
        "conv11 = Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), padding='same', kernel_initializer= 'random_normal', bias_initializer= 'zeros')(pool4)\n",
        "conv12 = Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), padding='same', kernel_initializer= 'random_normal', bias_initializer= 'zeros')(conv11)\n",
        "conv13 = Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), padding='same', kernel_initializer= 'random_normal', bias_initializer= 'zeros')(conv12)\n",
        "pool5 = MaxPooling2D(pool_size= (2,2), strides= 2)(conv13)\n",
        "flatten = Flatten()(pool5)\n",
        "dense1 = Dense(units=4096,activation='relu', kernel_regularizer= l2(0.0005), kernel_initializer= 'random_normal', bias_initializer= 'zeros')(flatten)\n",
        "dropout1 = Dropout(rate=0.5)(dense1)\n",
        "dense2 = Dense(units=4096, activation='relu', kernel_regularizer= l2(0.0005), kernel_initializer= 'random_normal', bias_initializer= 'zeros')(dropout1)\n",
        "dropout2 = Dropout(rate=0.5)(dense2)\n",
        "output = Dense(units=2, activation='sigmoid', kernel_regularizer= l2(0.0005), kernel_initializer= 'random_normal', bias_initializer= 'zeros')(dropout2)\n",
        "A = Model(inputs = input, outputs = output)\n",
        "A.summary()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oj-eNf_9F06o"
      },
      "source": [
        "opt = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False, name=\"Adam\")\n",
        "A.compile(optimizer= opt, metrics=['accuracy'], loss='categorical_crossentropy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tw09N0Nsc0hp"
      },
      "source": [
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if (logs.get('accuracy') >= 0.9):\n",
        "      print('\\nReached 90% accuracy so cancelling training!')\n",
        "      self.model.stop_training = True\n",
        "\n",
        "myCallback_object = myCallback()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_q2KTDXZG6Lj"
      },
      "source": [
        "model = A.fit(x=train_generator, validation_data= validation_generator, epochs=25, callbacks=myCallback_object, batch_size=148)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWsJcilhMMQk"
      },
      "source": [
        "Generating predictions using model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXttTrUcjPmf"
      },
      "source": [
        "test_generator.reset()\n",
        "y_pred = A.predict(test_generator)\n",
        "print(y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zir86_NHMQni"
      },
      "source": [
        "Converting Predictions to Labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCVZ9vljAdxL"
      },
      "source": [
        "predicted_class_indices=np.argmax(y_pred,axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uQij14KPHO-"
      },
      "source": [
        "names = (train_generator.class_indices)\n",
        "names = dict((v,k) for k,v in names.items())\n",
        "predictions = [names[k] for k in predicted_class_indices]\n",
        "print(predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IAdBFVXMjYi"
      },
      "source": [
        "Calculating and Visualizing Confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaGePIISI6e6"
      },
      "source": [
        "sns.heatmap(confusion_matrix(test_df['Label'], predictions), annot=True, cmap='inferno_r', square=True)\n",
        "plt.xlabel('Actual Values')\n",
        "plt.ylabel('Predicted Values')\n",
        "plt.title('Confusion Matrix')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVJy9xCByuJA"
      },
      "source": [
        "accuracy = accuracy_score(test_df['Label'], predictions)\n",
        "print(accuracy)\n",
        "\n",
        "precision = precision_score(test_df['Label'], predictions, average='micro')\n",
        "print(precision)\n",
        "\n",
        "\n",
        "recall = recall_score(test_df['Label'], predictions, average='micro')\n",
        "print(recall)\n",
        "\n",
        "f1 = f1_score(test_df['Label'], predictions, average='micro')\n",
        "print(f1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}